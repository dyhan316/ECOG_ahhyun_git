{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#주의 : 2048Hz!! 5s input window!!\n",
    "#therefore, must change accordingly!\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import glob\n",
    "import pandas as pd \n",
    "import utils\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import torch\n",
    "from omegaconf import OmegaConf\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from scipy.stats import ttest_ind\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# 전체 배열을 출력하도록 설정\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "\n",
    "#### SETUP ####\n",
    "include_fixation = True # False\n",
    "\n",
    "org_data_path = \"/global/cfs/cdirs/m4750/ECOG_AI/pilot_data/BrainBERT_preprocessed/task_data\"\n",
    "add_folder =  \"with_fixation_flattened\" if include_fixation else \"no_fixation_flattened\"\n",
    "data_path = os.path.join(org_data_path, add_folder)\n",
    "sampling_freq = 2000\n",
    "\n",
    "valid_split = 0.0\n",
    "test_split = 0.35 # minimum I think since there's only 120 samples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline 1. 정천기 교수님 랩 분석 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_stft(data, nperseg, clip_fs):\n",
    "    f, t, Zxx = utils.get_stft_multi_channel(data, fs=sampling_freq, clip_fs=clip_fs, batch_dim=True, \n",
    "                                             nperseg=nperseg, noverlap=nperseg-50, normalizing=\"zscore\", return_onesided=True)\n",
    "    return f, t, Zxx\n",
    "\n",
    "def apply_superlet(data):\n",
    "    return utils.get_superlet_multi_channel(data, fs=sampling_freq, nperseg=400, noverlap=350,\n",
    "                                            order_min=1, order_max=13, c_1=3, foi=np.arange(1,41), clip=5)\n",
    "\n",
    "def flatten_and_normalize(data, using_transform, using_segment, time_to_take):\n",
    "    if not using_transform:\n",
    "        flattened = data.reshape(data.shape[0], -1)\n",
    "        return (flattened - np.mean(flattened, axis=0)) / np.std(flattened, axis=0)\n",
    "    elif using_transform and not using_segment:\n",
    "        flattened = data[:,:,:,:time_to_take].reshape(data.shape[0], -1)\n",
    "        return flattened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Band: delta:\n",
      "Class 0 mean: 1.5532223558515687e-16, Class 1 mean: -1.6583593137463721e-15\n",
      "t-statistic: 1.8654113882911778\n",
      "p-value: 0.06985857077650229\n",
      "Band: theta:\n",
      "Class 0 mean: 2.557581836832819e-18, Class 1 mean: -1.2439148024595997e-17\n",
      "t-statistic: 0.5403996383305457\n",
      "p-value: 0.5920749668281431\n",
      "Band: alpha:\n",
      "Class 0 mean: 1.1160357106179583e-17, Class 1 mean: -1.860059517696601e-18\n",
      "t-statistic: 0.4149664366358154\n",
      "p-value: 0.680500079791103\n",
      "Band: beta:\n",
      "Class 0 mean: -2.208820677264709e-17, Class 1 mean: -1.429920754229259e-17\n",
      "t-statistic: -0.515151237836395\n",
      "p-value: 0.6094312931015778\n",
      "Band: low_gamma:\n",
      "Class 0 mean: 2.6389594407320474e-17, Class 1 mean: -6.51020831193809e-18\n",
      "t-statistic: 3.0535835804815643\n",
      "p-value: 0.004116475915604346\n",
      "Train set class 0 count: 22\n",
      "Train set class 1 count: 42\n",
      "Test set class 0 count: 6\n",
      "Test set class 1 count: 10\n",
      "Accuracy: 0.625\n",
      "AUC: 0.5\n",
      "Mean of test prediction: 1.0\n",
      "Band: high_gamma:\n",
      "Class 0 mean: 3.720119035393194e-18, Class 1 mean: 5.231417393521681e-18\n",
      "t-statistic: -0.2004800787033873\n",
      "p-value: 0.8421742786278398\n"
     ]
    }
   ],
   "source": [
    "def load_data(data_path):\n",
    "    labels_pd = pd.read_csv(os.path.join(data_path, \"labels.csv\")).sort_index()\n",
    "    train_pd, _, _ = utils.pd_train_val_test_split(labels_pd, valid_split=0, test_split=0, shuffle_seed=42)\n",
    "    return utils.arr_from_pd(data_path, train_pd)\n",
    "\n",
    "def sample_classes(y, n_samples=20, seed=42):\n",
    "    np.random.seed(seed)\n",
    "    class_0_indices = np.where(y == 0)[0]\n",
    "    class_1_indices = np.where(y == 1)[0]\n",
    "    sampled_class_0_indices = np.random.choice(class_0_indices, n_samples, replace=False)\n",
    "    sampled_class_1_indices = np.random.choice(class_1_indices, n_samples, replace=False)\n",
    "    return sampled_class_0_indices, sampled_class_1_indices\n",
    "\n",
    "def apply_transformations(X, indices, nperseg=4000, clip_fs=-1):\n",
    "    X_selected = X[indices]\n",
    "    f, t, X_stft = apply_stft(X_selected, nperseg=nperseg, clip_fs=clip_fs)\n",
    "    X_bandpower = utils.get_bandpower(X_stft)\n",
    "    return np.squeeze(X_bandpower, axis=1)\n",
    "\n",
    "def perform_ttest_and_svm(X_bandpower_ttest, X_bandpower_svm, y_remaining, sampled_class_0_indices, bandname_list):\n",
    "    t_stats, p_values = [], []\n",
    "    split_data_ttest = np.split(X_bandpower_ttest, X_bandpower_ttest.shape[1], axis=1)\n",
    "    split_data_svm = np.split(X_bandpower_svm, X_bandpower_svm.shape[1], axis=1)\n",
    "\n",
    "    for i, data in enumerate(split_data_ttest):\n",
    "        data = data.squeeze(axis=1)\n",
    "        X4ttest_class_0 = data[:len(sampled_class_0_indices)]\n",
    "        X4ttest_class_1 = data[len(sampled_class_0_indices):]\n",
    "\n",
    "        if np.isnan(X4ttest_class_0).any() or np.isnan(X4ttest_class_1).any():\n",
    "            print(f\"Band: {bandname_list[i]} contains NaN values.\")\n",
    "            X4ttest_class_0 = np.nan_to_num(X4ttest_class_0)\n",
    "            X4ttest_class_1 = np.nan_to_num(X4ttest_class_1)\n",
    "\n",
    "        if np.var(X4ttest_class_0) == 0 or np.var(X4ttest_class_1) == 0:\n",
    "            print(f\"Band: {bandname_list[i]} has zero variance.\")\n",
    "            t_stats.append(np.nan)\n",
    "            p_values.append(np.nan)\n",
    "            continue\n",
    "\n",
    "        mean_class_0 = np.mean(X4ttest_class_0, axis=1)\n",
    "        mean_class_1 = np.mean(X4ttest_class_1, axis=1)\n",
    "        t_stat, p_value = ttest_ind(mean_class_0, mean_class_1)\n",
    "        t_stats.append(t_stat)\n",
    "        p_values.append(p_value)\n",
    "\n",
    "        print(f\"Band: {bandname_list[i]}:\")\n",
    "        print(f\"Class 0 mean: {np.mean(mean_class_0)}, Class 1 mean: {np.mean(mean_class_1)}\")\n",
    "        print(\"t-statistic:\", t_stat)\n",
    "        print(\"p-value:\", p_value)\n",
    "\n",
    "        if p_value < 0.05:\n",
    "            data_svm = split_data_svm[i].squeeze(axis=1)\n",
    "            X_train, X_test, y_train, y_test = train_test_split(data_svm, y_remaining, test_size=0.2, random_state=42, stratify=y_remaining)\n",
    "            print(f\"Train set class 0 count: {np.sum(y_train == 0)}\")\n",
    "            print(f\"Train set class 1 count: {np.sum(y_train == 1)}\")\n",
    "\n",
    "            svm_model = SVC(kernel='rbf', C=1, gamma='scale', probability=True)\n",
    "            svm_model.fit(X_train, y_train)\n",
    "            y_pred = svm_model.predict(X_test)\n",
    "            y_prob = svm_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "            print(f\"Test set class 0 count: {np.sum(y_test == 0)}\")\n",
    "            print(f\"Test set class 1 count: {np.sum(y_test == 1)}\")\n",
    "            print(f\"Accuracy: {accuracy_score(y_test, y_pred)}\")\n",
    "            print(f\"AUC: {roc_auc_score(y_test, y_prob)}\")\n",
    "            print(f\"Mean of test prediction: {np.mean(y_pred)}\")\n",
    "\n",
    "    return t_stats, p_values\n",
    "\n",
    "def main(data_path):\n",
    "    X, y = load_data(data_path)\n",
    "    sampled_class_0_indices, sampled_class_1_indices = sample_classes(y)\n",
    "    remaining_indices = np.setdiff1d(np.arange(len(y)), np.concatenate([sampled_class_0_indices, sampled_class_1_indices]))\n",
    "    X_remaining, y_remaining = X[remaining_indices], y[remaining_indices]\n",
    "\n",
    "    X_bandpower_ttest = apply_transformations(X, np.concatenate([sampled_class_0_indices, sampled_class_1_indices]))\n",
    "    X_bandpower_svm = apply_transformations(X, remaining_indices)\n",
    "\n",
    "    bands = {\n",
    "        \"delta\": (0.5, 4),\n",
    "        \"theta\": (4, 8),\n",
    "        \"alpha\": (8, 13),\n",
    "        \"beta\": (13, 30),\n",
    "        \"low_gamma\": (30, 60),\n",
    "        \"high_gamma\": (60, 150)\n",
    "    }\n",
    "    bandname_list = list(bands.keys())\n",
    "\n",
    "    t_stats, p_values = perform_ttest_and_svm(X_bandpower_ttest, X_bandpower_svm, y_remaining, sampled_class_0_indices, bandname_list)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline 2. BrainBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/homes/s/seungju/.conda/envs/DIVER/lib/python3.12/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "# 현재 작업 디렉토리의 상위 디렉토리를 sys.path에 추가\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "import models\n",
    "\n",
    "# GPU 메모리 초기화 (필요하지 않음)\n",
    "# torch.cuda.empty_cache()\n",
    "\n",
    "def build_model(cfg):\n",
    "    ckpt_path = cfg.upstream_ckpt\n",
    "    init_state = torch.load(ckpt_path, map_location='cpu')  # CPU로 로드\n",
    "    upstream_cfg = init_state[\"model_cfg\"]\n",
    "    upstream = models.build_model(upstream_cfg)\n",
    "    return upstream\n",
    "\n",
    "def load_model_weights(model, states, multi_gpu):\n",
    "    if multi_gpu:\n",
    "        model.module.load_weights(states)\n",
    "    else:\n",
    "        model.load_weights(states)\n",
    "\n",
    "ckpt_path = \"../pretrained_weights/stft_large_pretrained.pth\"\n",
    "cfg = OmegaConf.create({\"upstream_ckpt\": ckpt_path})\n",
    "pretrained_model = build_model(cfg)\n",
    "pretrained_model.to('cpu')  # 모델을 CPU로 이동\n",
    "\n",
    "# 메모리 문제를 피하기 위해 map_location을 사용하여 모델 가중치 로드\n",
    "init_state = torch.load(ckpt_path, map_location='cpu')  # CPU로 로드\n",
    "load_model_weights(pretrained_model, init_state['model'], False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(X, segments):\n",
    "    split_points = np.cumsum([int(2000 * seg) for seg in segments])\n",
    "    split_data = [X[:, :, start:end] for start, end in zip([0] + split_points[:-1].tolist(), split_points)]\n",
    "    return split_data\n",
    "\n",
    "def process_data_BrainBERT(data_path, segments=5, using_transform=False, transform=None, time_to_take=None, using_timewiseavg=False, pooling=None, PCA_ncomp=None):\n",
    "    # Load labels and split into train, validation, and test sets\n",
    "    labels_pd = pd.read_csv(os.path.join(data_path, \"labels.csv\")).sort_index()\n",
    "    train_pd, valid_pd, test_pd = utils.pd_train_val_test_split(labels_pd, valid_split, test_split, shuffle_seed=42)\n",
    "\n",
    "    # Convert dataframes to arrays\n",
    "    X_train, y_train = utils.arr_from_pd(data_path, train_pd)\n",
    "    X_valid, y_valid = utils.arr_from_pd(data_path, valid_pd)\n",
    "    X_test, y_test = utils.arr_from_pd(data_path, test_pd)\n",
    "    \n",
    "    # Split data into segments\n",
    "    train_segments = split_data(X_train, segments)\n",
    "    test_segments = split_data(X_test, segments)\n",
    "\n",
    "    # Apply transformations\n",
    "    if using_transform:\n",
    "        if transform == 'stft':\n",
    "            train_segments = [apply_stft(segment, nperseg=400, clip_fs=40)[2] for segment in train_segments]\n",
    "            test_segments = [apply_stft(segment, nperseg=400, clip_fs=40)[2] for segment in test_segments]\n",
    "        elif transform == 'superlet':\n",
    "            train_segments = [apply_superlet(segment) for segment in train_segments]\n",
    "            test_segments = [apply_superlet(segment) for segment in test_segments]\n",
    "\n",
    "    # Apply BrainBERT\n",
    "    train_segments = [utils.make_ready_for_BrainBERT(segment, device='cpu') for segment in train_segments]\n",
    "    test_segments = [utils.make_ready_for_BrainBERT(segment, device='cpu') for segment in test_segments]\n",
    "\n",
    "    train_masks = [torch.zeros((segment.shape[:2])).bool().to('cpu') for segment in train_segments]\n",
    "    test_masks = [torch.zeros((segment.shape[:2])).bool().to('cpu') for segment in test_segments]\n",
    "\n",
    "    pretrained_model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_outputs = [pretrained_model.forward(segment, mask, intermediate_rep=True) for segment, mask in zip(train_segments, train_masks)]\n",
    "        test_outputs = [pretrained_model.forward(segment, mask, intermediate_rep=True) for segment, mask in zip(test_segments, test_masks)]\n",
    "\n",
    "        if pooling == 'max':\n",
    "            train_outputs = [F.max_pool1d(output.transpose(1, 2), kernel_size=10).transpose(1, 2) for output in train_outputs]\n",
    "            test_outputs = [F.max_pool1d(output.transpose(1, 2), kernel_size=10).transpose(1, 2) for output in test_outputs]\n",
    "        elif pooling == 'mean':\n",
    "            train_outputs = [F.avg_pool1d(output.transpose(1, 2), kernel_size=10).transpose(1, 2) for output in train_outputs]\n",
    "            test_outputs = [F.avg_pool1d(output.transpose(1, 2), kernel_size=10).transpose(1, 2) for output in test_outputs]\n",
    "        \n",
    "    train_segments = [utils.revert_BrainBERT_back(output) for output in train_outputs]\n",
    "    test_segments = [utils.revert_BrainBERT_back(output) for output in test_outputs]\n",
    "\n",
    "    if using_timewiseavg:\n",
    "        train_segments = [segment.mean(axis=-1, keepdims=True) for segment in train_segments]\n",
    "        test_segments = [segment.mean(axis=-1, keepdims=True) for segment in test_segments]\n",
    "        if valid_split != 0:\n",
    "            X_valid = X_valid.mean(axis=-1, keepdims=True)\n",
    "\n",
    "    train_segments_flattened = [segment.reshape(segment.shape[0], -1) for segment in train_segments]\n",
    "    test_segments_flattened = [segment.reshape(segment.shape[0], -1) for segment in test_segments]\n",
    "\n",
    "    if PCA_ncomp is not None:\n",
    "        train_segments_flattened_transformed = []\n",
    "        test_segments_flattened_transformed = []\n",
    "        for train_segment, test_segment in zip(train_segments_flattened, test_segments_flattened):\n",
    "            pca = PCA(n_components=PCA_ncomp)\n",
    "            train_segment_transformed = pca.fit_transform(train_segment)\n",
    "            test_segment_transformed = pca.transform(test_segment)\n",
    "            train_segments_flattened_transformed.append(train_segment_transformed)\n",
    "            test_segments_flattened_transformed.append(test_segment_transformed)\n",
    "        \n",
    "        train_segments_flattened = train_segments_flattened_transformed\n",
    "        test_segments_flattened = test_segments_flattened_transformed\n",
    "\n",
    "    train_segments_flattened = [torch.tensor(segment) for segment in train_segments_flattened]\n",
    "    test_segments_flattened = [torch.tensor(segment) for segment in test_segments_flattened]\n",
    "\n",
    "    train_data = torch.cat(train_segments_flattened, dim=1)\n",
    "    test_data = torch.cat(test_segments_flattened, dim=1)\n",
    "\n",
    "    if valid_split != 0:\n",
    "        X_valid_flattened = flatten_and_normalize(X_valid, using_transform, using_segment, time_to_take)\n",
    "        return train_data, y_train, X_valid_flattened, y_valid, test_data, y_test\n",
    "    else:\n",
    "        return train_data, y_train, None, None, test_data, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using stft transform with 191 time points\n",
      "Not using time-wise average\n",
      "Using max pooling\n",
      "5 sec divided by [5] sec\n",
      "==================================================\n",
      "X_train: torch.Size([78, 14592]) | X_test: torch.Size([42, 14592])\n",
      "No validation set\n",
      "==================================================\n",
      "LogisticRegression(max_iter=10000, random_state=42)\n",
      "Mean prediction: 0.7380952380952381\n",
      "Accuracy: 0.5714285714285714\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.27      0.23      0.25        13\n",
      "           1       0.68      0.72      0.70        29\n",
      "\n",
      "    accuracy                           0.57        42\n",
      "   macro avg       0.48      0.48      0.47        42\n",
      "weighted avg       0.55      0.57      0.56        42\n",
      "\n",
      "ROC AUC: 0.47745358090185674\n",
      "\n",
      "\n",
      "SVC(C=1, probability=True, random_state=42)\n",
      "Mean prediction: 1.0\n",
      "Accuracy: 0.6904761904761905\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        13\n",
      "           1       0.69      1.00      0.82        29\n",
      "\n",
      "    accuracy                           0.69        42\n",
      "   macro avg       0.35      0.50      0.41        42\n",
      "weighted avg       0.48      0.69      0.56        42\n",
      "\n",
      "ROC AUC: 0.5\n",
      "\n",
      "\n",
      "MLPClassifier(alpha=0.001, batch_size=32, early_stopping=True,\n",
      "              hidden_layer_sizes=(1024, 512, 256, 128),\n",
      "              learning_rate='adaptive', max_iter=1000, random_state=42,\n",
      "              validation_fraction=0.3)\n",
      "Mean prediction: 1.0\n",
      "Accuracy: 0.6904761904761905\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        13\n",
      "           1       0.69      1.00      0.82        29\n",
      "\n",
      "    accuracy                           0.69        42\n",
      "   macro avg       0.35      0.50      0.41        42\n",
      "weighted avg       0.48      0.69      0.56        42\n",
      "\n",
      "ROC AUC: 0.5\n",
      "\n",
      "\n",
      "Using stft transform with 191 time points\n",
      "Not using time-wise average\n",
      "Using mean pooling\n",
      "5 sec divided by [5] sec\n",
      "==================================================\n",
      "X_train: torch.Size([78, 14592]) | X_test: torch.Size([42, 14592])\n",
      "No validation set\n",
      "==================================================\n",
      "LogisticRegression(max_iter=10000, random_state=42)\n",
      "Mean prediction: 0.7857142857142857\n",
      "Accuracy: 0.5714285714285714\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.22      0.15      0.18        13\n",
      "           1       0.67      0.76      0.71        29\n",
      "\n",
      "    accuracy                           0.57        42\n",
      "   macro avg       0.44      0.46      0.45        42\n",
      "weighted avg       0.53      0.57      0.55        42\n",
      "\n",
      "ROC AUC: 0.4562334217506631\n",
      "\n",
      "\n",
      "SVC(C=1, probability=True, random_state=42)\n",
      "Mean prediction: 1.0\n",
      "Accuracy: 0.6904761904761905\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        13\n",
      "           1       0.69      1.00      0.82        29\n",
      "\n",
      "    accuracy                           0.69        42\n",
      "   macro avg       0.35      0.50      0.41        42\n",
      "weighted avg       0.48      0.69      0.56        42\n",
      "\n",
      "ROC AUC: 0.5\n",
      "\n",
      "\n",
      "MLPClassifier(alpha=0.001, batch_size=32, early_stopping=True,\n",
      "              hidden_layer_sizes=(1024, 512, 256, 128),\n",
      "              learning_rate='adaptive', max_iter=1000, random_state=42,\n",
      "              validation_fraction=0.3)\n",
      "Mean prediction: 1.0\n",
      "Accuracy: 0.6904761904761905\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        13\n",
      "           1       0.69      1.00      0.82        29\n",
      "\n",
      "    accuracy                           0.69        42\n",
      "   macro avg       0.35      0.50      0.41        42\n",
      "weighted avg       0.48      0.69      0.56        42\n",
      "\n",
      "ROC AUC: 0.5\n",
      "\n",
      "\n",
      "Using stft transform with 191 time points\n",
      "Not using time-wise average\n",
      "Not using pooling\n",
      "5 sec divided by [5] sec\n",
      "==================================================\n",
      "X_train: torch.Size([78, 146688]) | X_test: torch.Size([42, 146688])\n",
      "No validation set\n",
      "==================================================\n",
      "LogisticRegression(max_iter=10000, random_state=42)\n",
      "Mean prediction: 0.8333333333333334\n",
      "Accuracy: 0.5238095238095238\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        13\n",
      "           1       0.63      0.76      0.69        29\n",
      "\n",
      "    accuracy                           0.52        42\n",
      "   macro avg       0.31      0.38      0.34        42\n",
      "weighted avg       0.43      0.52      0.47        42\n",
      "\n",
      "ROC AUC: 0.3793103448275862\n",
      "\n",
      "\n",
      "SVC(C=1, probability=True, random_state=42)\n",
      "Mean prediction: 1.0\n",
      "Accuracy: 0.6904761904761905\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        13\n",
      "           1       0.69      1.00      0.82        29\n",
      "\n",
      "    accuracy                           0.69        42\n",
      "   macro avg       0.35      0.50      0.41        42\n",
      "weighted avg       0.48      0.69      0.56        42\n",
      "\n",
      "ROC AUC: 0.5\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/homes/s/seungju/.conda/envs/DIVER/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:697: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPClassifier(alpha=0.001, batch_size=32, early_stopping=True,\n",
      "              hidden_layer_sizes=(1024, 512, 256, 128),\n",
      "              learning_rate='adaptive', max_iter=1000, random_state=42,\n",
      "              validation_fraction=0.3)\n",
      "Mean prediction: 1.0\n",
      "Accuracy: 0.6904761904761905\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        13\n",
      "           1       0.69      1.00      0.82        29\n",
      "\n",
      "    accuracy                           0.69        42\n",
      "   macro avg       0.35      0.50      0.41        42\n",
      "weighted avg       0.48      0.69      0.56        42\n",
      "\n",
      "ROC AUC: 0.5\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "using_transform = True # True or False\n",
    "transform = 'stft' # 'stft' or 'superlet'\n",
    "time_to_take = 191 # How many points to take from the time axis when flattened\n",
    "using_timewiseavg = False\n",
    "pooling = ['max', 'mean', None] # 'max', 'mean', 'sum', None (kernel 사이즈는 위 process_data_BrainBERT 함수에서 수정해야 함)\n",
    "segments = [5] # 5초를 몇 초 간격으로 자를 것인지\n",
    "PCA_ncomp = None # PCA를 쓴다면 몇 개의 component를 사용할 것인지, None이면 PCA 사용 안 함\n",
    "\n",
    "for pooling_method in pooling:\n",
    "    print(f\"Using {transform} transform with {time_to_take} time points\") if using_transform else print(\"Not using transform\")\n",
    "    print(\"Using time-wise average\") if using_timewiseavg else print(\"Not using time-wise average\")\n",
    "    print(f\"Using {pooling_method} pooling\") if pooling_method != None else print(\"Not using pooling\")\n",
    "    print(f\"5 sec divided by {segments} sec\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    X_train, y_train, X_valid, y_valid, X_test, y_test = process_data_BrainBERT(data_path, segments, using_transform, transform, time_to_take, using_timewiseavg, pooling_method, PCA_ncomp)\n",
    "    print(f\"X_train: {X_train.shape} | X_test: {X_test.shape}\")\n",
    "    print(\"X_valid:\", X_valid.shape) if X_valid is not None else print(\"No validation set\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    clf_models = [\n",
    "        LogisticRegression(max_iter=10000, random_state=42), \n",
    "        SVC(kernel='rbf', C=1, gamma='scale', probability=True, random_state=42),\n",
    "        MLPClassifier(hidden_layer_sizes=(1024, 512, 256, 128), validation_fraction=0.3,\n",
    "                        learning_rate='adaptive', alpha=0.001, batch_size=32,\n",
    "                        early_stopping=True, max_iter=1000, random_state=42)    \n",
    "    ]\n",
    "\n",
    "    for model in clf_models:\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        print(model)\n",
    "        print(\"Mean prediction:\", y_pred.mean())\n",
    "        print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "        print(classification_report(y_test, y_pred, zero_division=0))\n",
    "        print(\"ROC AUC:\", roc_auc_score(y_test, y_pred))\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using stft transform with 191 time points\n",
      "Not using time-wise average\n",
      "Using max pooling\n",
      "5 sec divided by [2, 1.5, 1.5] sec\n",
      "==================================================\n",
      "X_train: torch.Size([78, 13056]) | X_test: torch.Size([42, 13056])\n",
      "No validation set\n",
      "==================================================\n",
      "LogisticRegression(max_iter=10000, random_state=42)\n",
      "Mean prediction: 0.8095238095238095\n",
      "Accuracy: 0.5952380952380952\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.25      0.15      0.19        13\n",
      "           1       0.68      0.79      0.73        29\n",
      "\n",
      "    accuracy                           0.60        42\n",
      "   macro avg       0.46      0.47      0.46        42\n",
      "weighted avg       0.54      0.60      0.56        42\n",
      "\n",
      "ROC AUC: 0.473474801061008\n",
      "\n",
      "\n",
      "SVC(C=1, probability=True, random_state=42)\n",
      "Mean prediction: 1.0\n",
      "Accuracy: 0.6904761904761905\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        13\n",
      "           1       0.69      1.00      0.82        29\n",
      "\n",
      "    accuracy                           0.69        42\n",
      "   macro avg       0.35      0.50      0.41        42\n",
      "weighted avg       0.48      0.69      0.56        42\n",
      "\n",
      "ROC AUC: 0.5\n",
      "\n",
      "\n",
      "MLPClassifier(alpha=0.001, batch_size=32, early_stopping=True,\n",
      "              hidden_layer_sizes=(1024, 512, 256, 128),\n",
      "              learning_rate='adaptive', max_iter=1000, random_state=42,\n",
      "              validation_fraction=0.3)\n",
      "Mean prediction: 1.0\n",
      "Accuracy: 0.6904761904761905\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        13\n",
      "           1       0.69      1.00      0.82        29\n",
      "\n",
      "    accuracy                           0.69        42\n",
      "   macro avg       0.35      0.50      0.41        42\n",
      "weighted avg       0.48      0.69      0.56        42\n",
      "\n",
      "ROC AUC: 0.5\n",
      "\n",
      "\n",
      "Using stft transform with 191 time points\n",
      "Not using time-wise average\n",
      "Using mean pooling\n",
      "5 sec divided by [2, 1.5, 1.5] sec\n",
      "==================================================\n",
      "X_train: torch.Size([78, 13056]) | X_test: torch.Size([42, 13056])\n",
      "No validation set\n",
      "==================================================\n",
      "LogisticRegression(max_iter=10000, random_state=42)\n",
      "Mean prediction: 0.8809523809523809\n",
      "Accuracy: 0.5714285714285714\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        13\n",
      "           1       0.65      0.83      0.73        29\n",
      "\n",
      "    accuracy                           0.57        42\n",
      "   macro avg       0.32      0.41      0.36        42\n",
      "weighted avg       0.45      0.57      0.50        42\n",
      "\n",
      "ROC AUC: 0.41379310344827586\n",
      "\n",
      "\n",
      "SVC(C=1, probability=True, random_state=42)\n",
      "Mean prediction: 1.0\n",
      "Accuracy: 0.6904761904761905\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        13\n",
      "           1       0.69      1.00      0.82        29\n",
      "\n",
      "    accuracy                           0.69        42\n",
      "   macro avg       0.35      0.50      0.41        42\n",
      "weighted avg       0.48      0.69      0.56        42\n",
      "\n",
      "ROC AUC: 0.5\n",
      "\n",
      "\n",
      "MLPClassifier(alpha=0.001, batch_size=32, early_stopping=True,\n",
      "              hidden_layer_sizes=(1024, 512, 256, 128),\n",
      "              learning_rate='adaptive', max_iter=1000, random_state=42,\n",
      "              validation_fraction=0.3)\n",
      "Mean prediction: 1.0\n",
      "Accuracy: 0.6904761904761905\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        13\n",
      "           1       0.69      1.00      0.82        29\n",
      "\n",
      "    accuracy                           0.69        42\n",
      "   macro avg       0.35      0.50      0.41        42\n",
      "weighted avg       0.48      0.69      0.56        42\n",
      "\n",
      "ROC AUC: 0.5\n",
      "\n",
      "\n",
      "Using stft transform with 191 time points\n",
      "Not using time-wise average\n",
      "Not using pooling\n",
      "5 sec divided by [2, 1.5, 1.5] sec\n",
      "==================================================\n",
      "X_train: torch.Size([78, 132864]) | X_test: torch.Size([42, 132864])\n",
      "No validation set\n",
      "==================================================\n",
      "LogisticRegression(max_iter=10000, random_state=42)\n",
      "Mean prediction: 0.7857142857142857\n",
      "Accuracy: 0.5238095238095238\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.11      0.08      0.09        13\n",
      "           1       0.64      0.72      0.68        29\n",
      "\n",
      "    accuracy                           0.52        42\n",
      "   macro avg       0.37      0.40      0.38        42\n",
      "weighted avg       0.47      0.52      0.50        42\n",
      "\n",
      "ROC AUC: 0.40053050397877976\n",
      "\n",
      "\n",
      "SVC(C=1, probability=True, random_state=42)\n",
      "Mean prediction: 1.0\n",
      "Accuracy: 0.6904761904761905\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        13\n",
      "           1       0.69      1.00      0.82        29\n",
      "\n",
      "    accuracy                           0.69        42\n",
      "   macro avg       0.35      0.50      0.41        42\n",
      "weighted avg       0.48      0.69      0.56        42\n",
      "\n",
      "ROC AUC: 0.5\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/homes/s/seungju/.conda/envs/DIVER/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:697: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPClassifier(alpha=0.001, batch_size=32, early_stopping=True,\n",
      "              hidden_layer_sizes=(1024, 512, 256, 128),\n",
      "              learning_rate='adaptive', max_iter=1000, random_state=42,\n",
      "              validation_fraction=0.3)\n",
      "Mean prediction: 1.0\n",
      "Accuracy: 0.6904761904761905\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        13\n",
      "           1       0.69      1.00      0.82        29\n",
      "\n",
      "    accuracy                           0.69        42\n",
      "   macro avg       0.35      0.50      0.41        42\n",
      "weighted avg       0.48      0.69      0.56        42\n",
      "\n",
      "ROC AUC: 0.5\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "using_transform = True # True or False\n",
    "transform = 'stft' # 'stft' or 'superlet'\n",
    "time_to_take = 191 # How many points to take from the time axis when flattened\n",
    "using_timewiseavg = False\n",
    "pooling = ['max', 'mean', None] # 'max', 'mean', 'sum', None (kernel 사이즈는 위 process_data_BrainBERT 함수에서 수정해야 함)\n",
    "segments = [2, 1.5, 1.5] # 5초를 몇 초 간격으로 자를 것인지\n",
    "PCA_ncomp = None # PCA를 쓴다면 몇 개의 component를 사용할 것인지, None이면 PCA 사용 안 함\n",
    "\n",
    "for pooling_method in pooling:\n",
    "    print(f\"Using {transform} transform with {time_to_take} time points\") if using_transform else print(\"Not using transform\")\n",
    "    print(\"Using time-wise average\") if using_timewiseavg else print(\"Not using time-wise average\")\n",
    "    print(f\"Using {pooling_method} pooling\") if pooling_method != None else print(\"Not using pooling\")\n",
    "    print(f\"5 sec divided by {segments} sec\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    X_train, y_train, X_valid, y_valid, X_test, y_test = process_data_BrainBERT(data_path, segments, using_transform, transform, time_to_take, using_timewiseavg, pooling_method, PCA_ncomp)\n",
    "    print(f\"X_train: {X_train.shape} | X_test: {X_test.shape}\")\n",
    "    print(\"X_valid:\", X_valid.shape) if X_valid is not None else print(\"No validation set\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    clf_models = [\n",
    "        LogisticRegression(max_iter=10000, random_state=42), \n",
    "        SVC(kernel='rbf', C=1, gamma='scale', probability=True, random_state=42),\n",
    "        MLPClassifier(hidden_layer_sizes=(1024, 512, 256, 128), validation_fraction=0.3,\n",
    "                        learning_rate='adaptive', alpha=0.001, batch_size=32,\n",
    "                        early_stopping=True, max_iter=1000, random_state=42)   \n",
    "    ]\n",
    "\n",
    "    for model in clf_models:\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        print(model)\n",
    "        print(\"Mean prediction:\", y_pred.mean())\n",
    "        print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "        print(classification_report(y_test, y_pred, zero_division=0))\n",
    "        print(\"ROC AUC:\", roc_auc_score(y_test, y_pred))\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using stft transform with 191 time points\n",
      "Not using time-wise average\n",
      "Using PCA pooling\n",
      "5 sec divided by (2, 1.5, 1.5) sec\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: torch.Size([78, 51]) | X_test: torch.Size([42, 51])\n",
      "No validation set\n",
      "==================================================\n",
      "LogisticRegression(max_iter=10000, random_state=42)\n",
      "Mean prediction: 0.5\n",
      "Accuracy: 0.42857142857142855\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.24      0.38      0.29        13\n",
      "           1       0.62      0.45      0.52        29\n",
      "\n",
      "    accuracy                           0.43        42\n",
      "   macro avg       0.43      0.42      0.41        42\n",
      "weighted avg       0.50      0.43      0.45        42\n",
      "\n",
      "ROC AUC: 0.41644562334217505\n",
      "\n",
      "\n",
      "SVC(C=1, probability=True, random_state=42)\n",
      "Mean prediction: 1.0\n",
      "Accuracy: 0.6904761904761905\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        13\n",
      "           1       0.69      1.00      0.82        29\n",
      "\n",
      "    accuracy                           0.69        42\n",
      "   macro avg       0.35      0.50      0.41        42\n",
      "weighted avg       0.48      0.69      0.56        42\n",
      "\n",
      "ROC AUC: 0.5\n",
      "\n",
      "\n",
      "MLPClassifier(alpha=0.001, batch_size=32, early_stopping=True,\n",
      "              hidden_layer_sizes=(1024, 512, 256, 128),\n",
      "              learning_rate='adaptive', max_iter=1000, random_state=42,\n",
      "              validation_fraction=0.3)\n",
      "Mean prediction: 1.0\n",
      "Accuracy: 0.6904761904761905\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        13\n",
      "           1       0.69      1.00      0.82        29\n",
      "\n",
      "    accuracy                           0.69        42\n",
      "   macro avg       0.35      0.50      0.41        42\n",
      "weighted avg       0.48      0.69      0.56        42\n",
      "\n",
      "ROC AUC: 0.5\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "using_transform = True # True or False\n",
    "transform = 'stft' # 'stft' or 'superlet'\n",
    "time_to_take = 191 # How many points to take from the time axis when flattened\n",
    "using_timewiseavg = False\n",
    "pooling = None # 'max', 'mean', 'sum', None (kernel 사이즈는 위 process_data_BrainBERT 함수에서 수정해야 함)\n",
    "segments = [2, 1.5, 1.5] # 5초를 몇 초 간격으로 자를 것인지\n",
    "PCA_ncomp = 10 # PCA를 쓴다면 몇 개의 component를 사용할 것인지, None이면 PCA 사용 안 함\n",
    "\n",
    "print(f\"Using {transform} transform with {time_to_take} time points\") if using_transform else print(\"Not using transform\")\n",
    "print(\"Using time-wise average\") if using_timewiseavg else print(\"Not using time-wise average\")\n",
    "print(f\"Using {pooling} pooling\") if pooling else print(\"Not using pooling\")\n",
    "print(f\"5 sec divided by {segments} sec\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "X_train, y_train, X_valid, y_valid, X_test, y_test = process_data_BrainBERT(data_path, segments, using_transform, transform, time_to_take, using_timewiseavg, pooling, PCA_ncomp)\n",
    "print(f\"X_train: {X_train.shape} | X_test: {X_test.shape}\")\n",
    "print(\"X_valid:\", X_valid.shape) if X_valid is not None else print(\"No validation set\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "clf_models = [\n",
    "    LogisticRegression(max_iter=10000, random_state=42), \n",
    "    SVC(kernel='rbf', C=1, gamma='scale', probability=True, random_state=42),\n",
    "    MLPClassifier(hidden_layer_sizes=(1024, 512, 256, 128), validation_fraction=0.3,\n",
    "                    learning_rate='adaptive', alpha=0.001, batch_size=32,\n",
    "                    early_stopping=True, max_iter=1000, random_state=42)   \n",
    "]\n",
    "\n",
    "for model in clf_models:\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(model)\n",
    "    print(\"Mean prediction:\", y_pred.mean())\n",
    "    print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "    print(classification_report(y_test, y_pred, zero_division=0))\n",
    "    print(\"ROC AUC:\", roc_auc_score(y_test, y_pred))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using stft transform with 191 time points\n",
      "Not using time-wise average\n",
      "Using max pooling\n",
      "5 sec divided by [1, 1, 1, 1, 1] sec\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: torch.Size([78, 11520]) | X_test: torch.Size([42, 11520])\n",
      "No validation set\n",
      "==================================================\n",
      "LogisticRegression(max_iter=10000, random_state=42)\n",
      "Mean prediction: 0.7857142857142857\n",
      "Accuracy: 0.6190476190476191\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.33      0.23      0.27        13\n",
      "           1       0.70      0.79      0.74        29\n",
      "\n",
      "    accuracy                           0.62        42\n",
      "   macro avg       0.52      0.51      0.51        42\n",
      "weighted avg       0.58      0.62      0.60        42\n",
      "\n",
      "ROC AUC: 0.5119363395225465\n",
      "\n",
      "\n",
      "SVC(C=1, probability=True, random_state=42)\n",
      "Mean prediction: 1.0\n",
      "Accuracy: 0.6904761904761905\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        13\n",
      "           1       0.69      1.00      0.82        29\n",
      "\n",
      "    accuracy                           0.69        42\n",
      "   macro avg       0.35      0.50      0.41        42\n",
      "weighted avg       0.48      0.69      0.56        42\n",
      "\n",
      "ROC AUC: 0.5\n",
      "\n",
      "\n",
      "MLPClassifier(alpha=0.001, batch_size=32, early_stopping=True,\n",
      "              hidden_layer_sizes=(1024, 512, 256, 128),\n",
      "              learning_rate='adaptive', max_iter=1000, random_state=42,\n",
      "              validation_fraction=0.3)\n",
      "Mean prediction: 1.0\n",
      "Accuracy: 0.6904761904761905\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        13\n",
      "           1       0.69      1.00      0.82        29\n",
      "\n",
      "    accuracy                           0.69        42\n",
      "   macro avg       0.35      0.50      0.41        42\n",
      "weighted avg       0.48      0.69      0.56        42\n",
      "\n",
      "ROC AUC: 0.5\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "using_transform = True # True or False\n",
    "transform = 'stft' # 'stft' or 'superlet'\n",
    "time_to_take = 191 # How many points to take from the time axis when flattened\n",
    "using_timewiseavg = False\n",
    "pooling = 'max' # 'max', 'mean', 'sum', None (kernel 사이즈는 위 process_data_BrainBERT 함수에서 수정해야 함)\n",
    "segments = [1, 1, 1, 1, 1] # 5초를 몇 초 간격으로 자를 것인지\n",
    "PCA_ncomp = None # PCA를 쓴다면 몇 개의 component를 사용할 것인지, None이면 PCA 사용 안 함\n",
    "\n",
    "print(f\"Using {transform} transform with {time_to_take} time points\") if using_transform else print(\"Not using transform\")\n",
    "print(\"Using time-wise average\") if using_timewiseavg else print(\"Not using time-wise average\")\n",
    "print(f\"Using {pooling} pooling\") if pooling else print(\"Not using pooling\")\n",
    "print(f\"5 sec divided by {segments} sec\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "X_train, y_train, X_valid, y_valid, X_test, y_test = process_data_BrainBERT(data_path, segments, using_transform, transform, time_to_take, using_timewiseavg, pooling, PCA_ncomp)\n",
    "print(f\"X_train: {X_train.shape} | X_test: {X_test.shape}\")\n",
    "print(\"X_valid:\", X_valid.shape) if X_valid is not None else print(\"No validation set\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "clf_models = [\n",
    "    LogisticRegression(max_iter=10000, random_state=42), \n",
    "    SVC(kernel='rbf', C=1, gamma='scale', probability=True, random_state=42),\n",
    "    MLPClassifier(hidden_layer_sizes=(1024, 512, 256, 128), validation_fraction=0.3,\n",
    "                    learning_rate='adaptive', alpha=0.001, batch_size=32,\n",
    "                    early_stopping=True, max_iter=1000, random_state=42)\n",
    "]\n",
    "\n",
    "for model in clf_models:\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(model)\n",
    "    print(\"Mean prediction:\", y_pred.mean())\n",
    "    print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "    print(classification_report(y_test, y_pred, zero_division=0))\n",
    "    print(\"ROC AUC:\", roc_auc_score(y_test, y_pred))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supple. BrainBERT 사용하지 않는 ver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(data_path, using_transform=False, transform=None, time_to_take=None, using_segment=False, seg_len=None, using_timewiseavg=False, using_bandpower=False, valid_split=0.2, test_split=0.2, sampling_freq=1000):\n",
    "    # 1. Load labels and split into train, validation, and test sets\n",
    "    labels_pd = pd.read_csv(os.path.join(data_path, \"labels.csv\")).sort_index()\n",
    "    train_pd, valid_pd, test_pd = utils.pd_train_val_test_split(labels_pd, valid_split, test_split, shuffle_seed=42)\n",
    "\n",
    "    # 2. Convert dataframes to arrays\n",
    "    X_train, y_train = utils.arr_from_pd(data_path, train_pd)\n",
    "    X_valid, y_valid = utils.arr_from_pd(data_path, valid_pd)\n",
    "    X_test, y_test = utils.arr_from_pd(data_path, test_pd)\n",
    "\n",
    "    # 3. Apply transformations\n",
    "    if using_transform:\n",
    "        if transform == 'stft':\n",
    "            X_train = apply_stft(X_train, nperseg=400, clip_fs=5, sampling_freq=sampling_freq)\n",
    "            X_test = apply_stft(X_test, nperseg=400, clip_fs=5, sampling_freq=sampling_freq)\n",
    "            if valid_split != 0:\n",
    "                X_valid = apply_stft(X_valid, nperseg=400, clip_fs=5, sampling_freq=sampling_freq)\n",
    "        elif transform == 'superlet':\n",
    "            X_train = apply_superlet(X_train, sampling_freq=sampling_freq)\n",
    "            X_test = apply_superlet(X_test, sampling_freq=sampling_freq)\n",
    "            if valid_split != 0:\n",
    "                X_valid = apply_superlet(X_valid, sampling_freq=sampling_freq)\n",
    "        print(f\"Transformed shape: X train: {X_train.shape}, y train: {y_train.shape}, X test: {X_test.shape}, y test: {y_test.shape}\")\n",
    "    \n",
    "    # 4. Segment data if required\n",
    "    if using_segment:\n",
    "        X_train, y_train = utils.segment_data_and_labels(X_train, y_train, seg_len)\n",
    "        X_test, y_test = utils.segment_data_and_labels(X_test, y_test, seg_len)\n",
    "        if valid_split != 0:\n",
    "            X_valid, y_valid = utils.segment_data_and_labels(X_valid, y_valid, seg_len)\n",
    "        print(f\"Segmented shape: X train: {X_train.shape}, y train: {y_train.shape}, X test: {X_test.shape}, y test: {y_test.shape}\")        \n",
    "    \n",
    "    # 5. Apply time-wise averaging if required\n",
    "    if using_timewiseavg:\n",
    "        X_train = X_train.mean(axis=-1, keepdims=True)\n",
    "        X_test = X_test.mean(axis=-1, keepdims=True)\n",
    "        if valid_split != 0:\n",
    "            X_valid = X_valid.mean(axis=-1, keepdims=True)\n",
    "\n",
    "    # 6. Apply bandpower transformation if required\n",
    "    if using_transform and using_bandpower:\n",
    "        X_train = utils.get_bandpower(X_train)\n",
    "        X_test = utils.get_bandpower(X_test)\n",
    "        if valid_split != 0:\n",
    "            X_valid = utils.get_bandpower(X_valid)\n",
    "        print(f\"Bandpower shape: X train: {X_train.shape}, y train: {y_train.shape}, X test: {X_test.shape}, y test: {y_test.shape}\")\n",
    "   \n",
    "    # 7. Flatten and normalize data\n",
    "    X_train_flattened = flatten_and_normalize(X_train, using_transform, using_segment, time_to_take)\n",
    "    X_test_flattened = flatten_and_normalize(X_test, using_transform, using_segment, time_to_take)\n",
    "\n",
    "    # 8. Print final shapes and return data\n",
    "    print(f\"Input shape: X train: {X_train_flattened.shape}, X test: {X_test_flattened.shape}\")\n",
    "    print(f\"Train set 0: {np.sum(y_train == 0)}, Train set 1: {np.sum(y_train == 1)}\")\n",
    "    print(f\"Test set 0: {np.sum(y_test == 0)}, Test set 1: {np.sum(y_test == 1)}\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    if valid_split != 0:\n",
    "        X_valid_flattened = flatten_and_normalize(X_valid, using_transform, using_segment, time_to_take)\n",
    "        return X_train_flattened, y_train, X_valid_flattened, y_valid, X_test_flattened, y_test\n",
    "    else:\n",
    "        return X_train_flattened, y_train, None, None, X_test_flattened, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not using transform\n",
      "Not using segment\n",
      "Not using time-wise average\n",
      "Using bandpower\n",
      "==================================================\n",
      "Using bandpower: False\n",
      "(78, 1, 10000)\n",
      "(42, 1, 10000)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m using_bandpower_bool \u001b[38;5;129;01min\u001b[39;00m using_bandpower:\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing bandpower: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00musing_bandpower_bool\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 20\u001b[0m     X_train, y_train, X_valid, y_valid, X_test, y_test \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musing_transform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtime_to_take\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musing_segment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseg_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43musing_timewiseavg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musing_bandpower_bool\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m     clf_models \u001b[38;5;241m=\u001b[39m [LogisticRegression(max_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10000\u001b[39m), \n\u001b[1;32m     23\u001b[0m           MLPClassifier(hidden_layer_sizes\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1024\u001b[39m, \u001b[38;5;241m512\u001b[39m, \u001b[38;5;241m256\u001b[39m, \u001b[38;5;241m128\u001b[39m), validation_fraction\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.3\u001b[39m,\n\u001b[1;32m     24\u001b[0m                         learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madaptive\u001b[39m\u001b[38;5;124m'\u001b[39m, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m,\n\u001b[1;32m     25\u001b[0m                         early_stopping\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, max_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m)]\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m clf_models:\n",
      "Cell \u001b[0;32mIn[10], line 69\u001b[0m, in \u001b[0;36mprocess_data\u001b[0;34m(data_path, using_transform, transform, time_to_take, using_segment, seg_len, using_timewiseavg, using_bandpower)\u001b[0m\n\u001b[1;32m     66\u001b[0m X_train_flattened \u001b[38;5;241m=\u001b[39m flatten_and_normalize(X_train, using_transform, using_segment, time_to_take)\n\u001b[1;32m     67\u001b[0m X_test_flattened \u001b[38;5;241m=\u001b[39m flatten_and_normalize(X_test, using_transform, using_segment, time_to_take)\n\u001b[0;32m---> 69\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput shape: X train: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mX_train_flattened\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, X test: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX_test_flattened\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain set 0: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39msum(y_train\u001b[38;5;250m \u001b[39m\u001b[38;5;241m==\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Train set 1: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39msum(y_train\u001b[38;5;250m \u001b[39m\u001b[38;5;241m==\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest set 0: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39msum(y_test\u001b[38;5;250m \u001b[39m\u001b[38;5;241m==\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Test set 1: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39msum(y_test\u001b[38;5;250m \u001b[39m\u001b[38;5;241m==\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "using_transform = False # True or False\n",
    "transform = 'stft' # 'stft' or 'superlet'\n",
    "time_to_take = 191 # How many points to take from the time axis when flattened\n",
    "using_segment = False # True or False\n",
    "seg_len = 10\n",
    "using_timewiseavg = False\n",
    "using_bandpower = [False, True] # if using_transform is False, this will be ignored\n",
    "\n",
    "print(f\"Using {transform} transform with {time_to_take} time points\") if using_transform else print(\"Not using transform\")\n",
    "print(f\"Using segment length {seg_len}\") if using_segment else print(\"Not using segment\")\n",
    "print(\"Using time-wise average\") if using_timewiseavg else print(\"Not using time-wise average\")\n",
    "print(\"Using bandpower\") if using_bandpower else print(\"Not using bandpower\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for using_bandpower_bool in using_bandpower:\n",
    "    print(f\"Using bandpower: {using_bandpower_bool}\")\n",
    "    \n",
    "    X_train, y_train, X_valid, y_valid, X_test, y_test = process_data(data_path, using_transform, transform, time_to_take, using_segment, seg_len,  using_timewiseavg, using_bandpower_bool)\n",
    "    \n",
    "    clf_models = [LogisticRegression(max_iter=10000), \n",
    "          MLPClassifier(hidden_layer_sizes=(1024, 512, 256, 128), validation_fraction=0.3,\n",
    "                        learning_rate='adaptive', alpha=0.001, batch_size=32,\n",
    "                        early_stopping=True, max_iter=1000)]\n",
    "    \n",
    "    for model in clf_models:\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        print(model)\n",
    "        print(\"Mean prediction:\", y_pred.mean())\n",
    "        print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "        print(classification_report(y_test, y_pred, zero_division=0))\n",
    "        print(\"ROC AUC:\", roc_auc_score(y_test, y_pred))\n",
    "        print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DIVER",
   "language": "python",
   "name": "diver"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
